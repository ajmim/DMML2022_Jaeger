{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CamamBERT model\n",
    "\n",
    "> CamemBERT is a state-of-the-art language model for French based on the RoBERTa architecture pretrained on the French subcorpus of the newly available multilingual corpus OSCAR.\n",
    "> https://camembert-model.fr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing and functions\n",
    "This cell imports and defines all the important libraries and functions for the Camambert model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import seaborn as sns\n",
    "import torch  # GPU optim. + gradient opt.\n",
    "from torch.utils.data import DataLoader\n",
    "import functools\n",
    "from LightningModel import LightningModel\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Takes a batch and process the text.\n",
    "def tokenize_batch(samples, tokenizer):\n",
    "    text = [sample[\"sentence\"] for sample in samples]\n",
    "    labels = torch.tensor([sample[\"label\"] for sample in samples])\n",
    "    str_labels = [sample[\"difficulty\"] for sample in samples]\n",
    "    # The tokenizer handles\n",
    "    # - Tokenization (amazing right?)\n",
    "    # - Padding (adding empty tokens so that each example has the same length)\n",
    "    # - Truncation (cutting samples that are too long)\n",
    "    # - Special tokens (in CamemBERT, each sentence ends with a special token </s>)\n",
    "    # - Attention mask (a binary vector which tells the model which tokens to look at. For instance it will not compute anything if the token is a padding token)\n",
    "    tokens = tokenizer(text, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "    return {\"input_ids\": tokens.input_ids, \"attention_mask\": tokens.attention_mask, \"labels\": labels, \"str_labels\": str_labels, \"sentences\": text}\n",
    "\n",
    "# Once the model is trained, this method will return the confusion matrix.\n",
    "def plot_confusion_matrix(labels, preds, label_names):\n",
    "    confusion_norm = confusion_matrix(labels, preds.tolist(), labels=list(range(len(label_names))), normalize=\"true\")\n",
    "    confusion = confusion_matrix(labels, preds.tolist(), labels=list(range(len(label_names))))\n",
    "\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    sns.heatmap(\n",
    "        confusion_norm,\n",
    "        annot=confusion,\n",
    "        cbar=False,\n",
    "        fmt=\"d\",\n",
    "        xticklabels=label_names,\n",
    "        yticklabels=label_names,\n",
    "        cmap=\"viridis\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenize and import data\n",
    "**STEP 1 -  Create the tokenizer for the data**\n",
    "The AutoTokenizer breaks the text into words and phrases. It also clean the data and preprocess it as our CamamBERT model need.\n",
    "\n",
    "**STEP 2 - load the data**\n",
    "We need a total of  3 sets:\n",
    "1. a train dataset for training the model.\n",
    "2. a validation dataset for fine tune the result across epochs.\n",
    "3. a testing set, to evaluate our model once all epochs are completed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Breaking up + cleaning + processing the text\n",
    "tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "# Loading and setting the datasets\n",
    "dataset = load_dataset('Makxxx/french_CEFR') # stocked in huggingface in a form of a dictionary. It contains all 3 datasets.\n",
    "\n",
    "pd_dataset = {split_name: split_data.to_pandas() for split_name, split_data in dataset.items()} # Setting up the validation set\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = dataset.values() #taking the values from the dataset (dictionary) and attributing them to new variables.\n",
    "\n",
    "num_labels = len(pd_dataset[\"train\"][\"label\"].unique()) # Saving the number of classes from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualize data\n",
    "\n",
    "You can find here plots and prints to better understand the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This plot shows the labels and their frequencies\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "nb_labels = len(pd_dataset[\"train\"][\"label\"].unique())\n",
    "print(f\"Le dataset comprend {nb_labels} labels.\")\n",
    "\n",
    "ax = pd_dataset[\"train\"][\"label\"].hist(density=True, bins=nb_labels)\n",
    "ax.set_xlabel(\"Label ID\")\n",
    "ax.set_ylabel(\"Fréquence\")\n",
    "ax.set_title(\"Répartition des labels dans le dataset (train split)\")\n",
    "ax.figure.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This graph shows the length of senteces (number of characters).\n",
    "\n",
    "pd_dataset[\"train\"][\"len_sen\"] = pd_dataset[\"train\"][\"sentence\"].apply(lambda x: len(x))\n",
    "ax = pd_dataset[\"train\"][\"len_sen\"].hist(density=True, bins=50)\n",
    "ax.set_xlabel(\"Longueur\")\n",
    "ax.set_ylabel(\"Fréquence\")\n",
    "ax.set_title(\"Nombre de caractères par phrase\")\n",
    "ax.figure.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Some addItionnal information\n",
    "\n",
    "print(\"Max lenght of a sentence: \", pd_dataset[\"train\"][\"len_sen\"].max())\n",
    "print(\"Number of rows in the training set: \", train_dataset.shape[0])\n",
    "print(\"Number of rows in the testing set: \", test_dataset.shape[0])\n",
    "print(\"Number of rows in the validation set: \", val_dataset.shape[0])\n",
    "\n",
    "'''\n",
    "ADD IN DOCUMENTATIN HOW WE ADDED MORE DATA TO IMPROVE MODEL -----------------!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load the DATA in the batches and define parameters.\n",
    "the Dataloader has 4 parameters:\n",
    "- The dataset.\n",
    "- Batches are packs of data we inject while training the model to decrease the load on the processor and GPU. The bigger the batch size is, the faster the training is supposed to go. In case the batch size is big, the hardware must have enough memory to load the data.\n",
    "- shuffle: move data around to prevent the model to remember the exact dataset to prevent the data to be too specific to the dataset rendering it useless for other ones.\n",
    "- collate_fn: tells how to put together the data into the batch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# link the dataset to the different models. We set up the batch and random parameters.\n",
    "train_dataloader = DataLoader(\n",
    "    dataset[\"train\"],\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer) #uses the function and tokenizer declared above.\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset[\"validation\"],\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the model\n",
    "Creation of LightingModel instance imported from lightning_model file. This is the model that will be used to train the dataset. They have as attributes:\n",
    "- Model Name (camembert-base).\n",
    "- Number of labels (num_labels).\n",
    "- lr as the step the gradient takes every epoch to optimize the solution.\n",
    "- weight decay which increases generalization. The gradient would depend on it which will decrease the dependence on the training set.\n",
    "\n",
    "Other parameters:\n",
    "- max epochs: how many iterations to train the model.\n",
    "- gpus: number of GPUs running the model.\n",
    "- callbacks: in case the gradient is not optimizing the result enough, the callbacks would define when to stop it to return the result.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lightning_model = LightningModel(\"camembert-base\", num_labels, lr=3e-5, weight_decay=2)\n",
    "\n",
    "model_checkpoint = pl.callbacks.ModelCheckpoint(monitor=\"valid/acc\", mode=\"max\")\n",
    "\n",
    "camembert_trainer = pl.Trainer(\n",
    "    max_epochs=25, #how many times iteration on dataset.\n",
    "    gpus=1,\n",
    "    callbacks=[\n",
    "        pl.callbacks.EarlyStopping(monitor=\"valid/acc\", patience=4, mode=\"max\"),\n",
    "        model_checkpoint,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# fit the model\n",
    "camembert_trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)\n",
    "\n",
    "# recover best model we found, usually between 5-10.\n",
    "lightning_model = LightningModel.load_from_checkpoint(checkpoint_path=model_checkpoint.best_model_path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysing training results.\n",
    "For now, we only compared the training set to the evaluation one. In this section, we continue to compared them together. We will look at:\n",
    "- The Confusion matrix\n",
    "- Examples of wrong classifications."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we ecode the classification classes. This would be useful once we want to look at which sentence is wrongly classified.\n",
    "We also add the classes names to a list to use them easily.\n",
    "\n",
    "Secondly, we create the prediction variable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ID_TO_LABEL = dict(zip(range(6), ('A1', 'A2', 'B1', 'B2', 'C1', 'C2',)))\n",
    "label_names = list(ID_TO_LABEL.values())\n",
    "\n",
    "camembert_preds = camembert_trainer.predict(lightning_model, dataloaders=val_dataloader)\n",
    "camembert_preds = torch.cat(camembert_preds, -1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the Confusion Matrix."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_confusion_matrix(dataset[\"validation\"][\"label\"], camembert_preds, label_names)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the precision, recall, f1-score for each class and for the model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# classification method is defined above to provide the report of relevant metrics.\n",
    "print(classification_report(dataset[\"validation\"][\"label\"], camembert_preds, target_names=label_names))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Printing some examples of wrongly classified sentenses."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wrong_preds = camembert_preds.numpy() != np.array(dataset[\"validation\"][\"label\"])\n",
    "wrong = dataset[\"validation\"].to_pandas()[['sentence', 'difficulty']][wrong_preds]\n",
    "\n",
    "preds = pd.Series(camembert_preds.numpy())[wrong_preds].apply(lambda x: ID_TO_LABEL[x])\n",
    "wrong[\"preds\"] = preds\n",
    "wrong.columns = [\"sentence\", \"true\", \"predicted\"]\n",
    "wrong"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating final dataframe for submission.\n",
    "\n",
    "Using the same logic as above, we load the test dataset and predict the outcome taking the trained model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Loading the dataset\n",
    "test_dataloader = DataLoader(\n",
    "    dataset[\"test\"],\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer)\n",
    ")\n",
    "\n",
    "#Predicts and format using trained model.\n",
    "preds = camembert_trainer.predict(lightning_model, dataloaders=test_dataloader)\n",
    "preds = torch.cat(preds, -1)\n",
    "\n",
    "# format the data for submission\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "test_df.label = preds.numpy()\n",
    "test_df.difficulty = test_df.label.apply(lambda x: label_names[x])\n",
    "test_df.index.name = 'id'\n",
    "test_df.drop(columns=[\"sentence\", \"label\"], inplace=True)\n",
    "\n",
    "#Generate the csv file\n",
    "test_df.to_csv('preds.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}